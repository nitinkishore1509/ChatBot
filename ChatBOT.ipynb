{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 0 - Install libraries\n",
    "# -------------------------\n",
    "!pip install -q pdfplumber sentence-transformers faiss-cpu transformers gradio nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 1 - Imports\n",
    "# -------------------------\n",
    "import pdfplumber\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import pipeline\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Step 2 - Download NLTK data\n",
    "# -------------------------\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\", quiet=True)  # Fix for newer NLTK versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Step 3 - Load Models (once at startup)\n",
    "# -------------------------\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Fast embeddings model\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")  # Small Q&A model\n",
    "\n",
    "# Warmup models for faster first response\n",
    "_ = embedder.encode([\"Warmup\"], convert_to_tensor=False)\n",
    "_ = generator(\"This is a warmup test.\", max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 4 - PDF Reading\n",
    "# -------------------------\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF, skipping empty pages.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 5 - Text Chunking\n",
    "# -------------------------\n",
    "def chunk_text(text, chunk_size=200):\n",
    "    \"\"\"Split text into smaller chunks for retrieval.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "    for sent in sentences:\n",
    "        if current_len + len(sent.split()) > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_len = 0\n",
    "        current_chunk.append(sent)\n",
    "        current_len += len(sent.split())\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 6 - Create FAISS Index\n",
    "# -------------------------\n",
    "def create_faiss_index(chunks):\n",
    "    \"\"\"Embed chunks and store in FAISS index.\"\"\"\n",
    "    embeddings = embedder.encode(chunks, convert_to_tensor=False)\n",
    "    dim = len(embeddings[0])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 7 - Retrieve Relevant Chunks\n",
    "# -------------------------\n",
    "def retrieve(query, index, chunks, top_k=3):\n",
    "    \"\"\"Retrieve top relevant chunks for the question.\"\"\"\n",
    "    query_embedding = embedder.encode([query], convert_to_tensor=False)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 8 - Generate Answer\n",
    "# -------------------------\n",
    "def generate_answer(question, context):\n",
    "    \"\"\"Use the generator model to answer from context.\"\"\"\n",
    "    prompt = f\"Answer the question using the context below:\\n\\nContext: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    result = generator(prompt, max_length=150, clean_up_tokenization_spaces=True)\n",
    "    return result[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 9 - Full Pipeline\n",
    "# -------------------------\n",
    "def process_pdf_and_answer(pdf_file, question):\n",
    "    # Handle both file object and string path\n",
    "    if isinstance(pdf_file, str):\n",
    "        file_path = pdf_file\n",
    "    else:\n",
    "        file_path = pdf_file.name\n",
    "\n",
    "    text = read_pdf(file_path)\n",
    "    if not text:\n",
    "        return \"No text could be extracted from this PDF. It might be scanned or image-based.\"\n",
    "\n",
    "    chunks = chunk_text(text)\n",
    "    index, chunks_list = create_faiss_index(chunks)\n",
    "    top_chunks = retrieve(question, index, chunks_list)\n",
    "    context = \" \".join(top_chunks)\n",
    "    answer = generate_answer(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://19bfeaef825fc701e0.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://19bfeaef825fc701e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Step 10 - Gradio UI\n",
    "# -------------------------\n",
    "def gradio_interface(pdf_file, question):\n",
    "    if not pdf_file or not question:\n",
    "        return \"Please upload a PDF and ask a question.\"\n",
    "    return process_pdf_and_answer(pdf_file, question)\n",
    "\n",
    "ui = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload your Course Notes PDF\", file_types=[\".pdf\"]),\n",
    "        gr.Textbox(label=\"Ask a Question\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Course Notes Q&A Chatbot\",\n",
    "    description=\"Upload your PDF notes and ask questions!\"\n",
    ")\n",
    "\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvAwTlOO3tnU"
   },
   "source": [
    "# New Section"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
